"""
AIåˆ†ææ¨¡å— - ä½¿ç”¨LangChainè¿›è¡ŒåŸºé‡‘å¸‚åœºæ·±åº¦åˆ†æ

è¯¥æ¨¡å—æä¾›åŸºäºLangChainçš„AIåˆ†æåŠŸèƒ½ï¼ŒåŒ…æ‹¬ï¼š
- å¸‚åœºè¶‹åŠ¿åˆ†æ
- æ¿å—æœºä¼šåˆ†æ
- åŸºé‡‘ç»„åˆå»ºè®®
- é£é™©æç¤ºåˆ†æ
"""

import os
import time
import datetime
import re
from loguru import logger


from langchain.tools import tool
from ddgs import DDGS

@tool
def search_news(query: str) -> str:
    """ä½¿ç”¨DuckDuckGoæœç´¢æ–°é—»å†…å®¹ï¼ˆé™åˆ¶æœ€è¿‘ä¸€å‘¨ï¼‰

    Args:
        query: æœç´¢å…³é”®è¯
    """
    try:
        # è§£æå‚æ•°ï¼ˆæ”¯æŒç›´æ¥ä¼ å…¥å­—ç¬¦ä¸²æˆ–JSONå­—ç¬¦ä¸²ï¼‰
        import json
        if isinstance(query, str):
            if query.strip().startswith('{'):
                # å¦‚æœæ˜¯JSONæ ¼å¼: {"query": "å…³é”®è¯"}
                try:
                    parsed = json.loads(query)
                    query = parsed.get('query', '')
                except:
                    pass  # ä¿æŒåŸå§‹query

        ddgs = DDGS(verify=False)
        results = ddgs.text(
            query=query,
            region="cn-zh",
            safesearch="off",
            timelimit="w",  # é™åˆ¶æœ€è¿‘ä¸€å‘¨
            max_results=10,
        )

        if not results:
            return f"æœªæ‰¾åˆ°å…³äº'{query}'çš„ç›¸å…³æ–°é—»"

        output = f"å…³äº'{query}'çš„æœç´¢ç»“æœï¼ˆæœ€è¿‘ä¸€å‘¨ï¼‰ï¼š\n\n"
        for i, result in enumerate(results, 1):
            title = result.get("title", "æ— æ ‡é¢˜")
            body = result.get("body", "æ— å†…å®¹")
            url = result.get("href", "")
            output += f"{i}. {title}\n{body}\næ¥æº: {url}\n\n"

        return output
    except Exception as e:
        return f"æœç´¢å¤±è´¥: {str(e)}"

class AIAnalyzer:
    """AIåˆ†æå™¨ï¼Œæä¾›åŸºäºLangChainçš„å¸‚åœºåˆ†æåŠŸèƒ½"""

    def __init__(self):
        """åˆå§‹åŒ–AIåˆ†æå™¨"""
        self.llm = None

    def init_langchain_llm(self, fast_mode=False, deep_mode=False):
        """
        åˆå§‹åŒ–LangChain LLM

        Args:
            fast_mode: æ˜¯å¦ä¸ºå¿«é€Ÿæ¨¡å¼ï¼ˆè°ƒæ•´tokenå’Œè¶…æ—¶å‚æ•°ï¼‰
            deep_mode: æ˜¯å¦ä¸ºæ·±åº¦ç ”ç©¶æ¨¡å¼ï¼ˆå¤§å¹…æå‡tokené™åˆ¶ä»¥æ”¯æŒé•¿æŠ¥å‘Šç”Ÿæˆï¼‰
        """
        try:
            from langchain_openai import ChatOpenAI

            # ä»ç¯å¢ƒå˜é‡è¯»å–é…ç½®
            api_base = os.getenv("LLM_API_BASE", "https://api.openai.com/v1")
            api_key = os.getenv("LLM_API_KEY", "")
            model = os.getenv("LLM_MODEL", "gpt-3.5-turbo")

            if not api_key:
                logger.warning("æœªé…ç½®LLM_API_KEYç¯å¢ƒå˜é‡ï¼Œè·³è¿‡AIåˆ†æ")
                return None

            # æ ¹æ®æ¨¡å¼è°ƒæ•´å‚æ•°
            if fast_mode:
                max_tokens = 1000
                temperature = 0.2
                timeout = 30
            elif deep_mode:
                # æ·±åº¦ç ”ç©¶æ¨¡å¼ï¼šæ”¯æŒç”Ÿæˆ10,000+å­—çš„é•¿æŠ¥å‘Š
                max_tokens = 16000
                temperature = 0.2
                timeout = 120
            else:
                max_tokens = 2000
                temperature = 0.2
                timeout = 60

            # åˆ›å»ºChatOpenAIå®ä¾‹
            llm = ChatOpenAI(
                model=model,
                openai_api_key=api_key,
                openai_api_base=api_base,
                temperature=temperature,
                max_tokens=max_tokens,
                request_timeout=timeout
            )

            return llm

        except Exception as e:
            logger.error(f"åˆå§‹åŒ–LangChain LLMå¤±è´¥: {e}")
            return None

    @staticmethod
    def clean_ansi_codes(text):
        """æ¸…ç†æ‰€æœ‰ANSIé¢œè‰²ä»£ç """
        if not isinstance(text, str):
            return text
        # æ¸…ç†å®Œæ•´çš„ANSIè½¬ä¹‰åºåˆ— \033[XXXm
        text = re.sub(r'\033\[\d+(?:;\d+)?m', '', text)
        # æ¸…ç†ä¸å®Œæ•´çš„ANSIä»£ç  [XXXm (å¯èƒ½åœ¨æŸäº›æƒ…å†µä¸‹\033è¢«æˆªæ–­)
        text = re.sub(r'\[\d+(?:;\d+)?m', '', text)
        return text

    @staticmethod
    def strip_markdown(text):
        """ç§»é™¤markdownæ ¼å¼æ ‡è®°ï¼Œç”¨äºæ§åˆ¶å°æ˜¾ç¤º"""
        # ç§»é™¤æ ‡é¢˜ç¬¦å· (###ã€##ã€#)
        text = re.sub(r'^#{1,6}\s+', '', text, flags=re.MULTILINE)

        # ç§»é™¤åŠ ç²— (**text** æˆ– __text__)
        text = re.sub(r'\*\*(.+?)\*\*', r'\1', text)
        text = re.sub(r'__(.+?)__', r'\1', text)

        # ç§»é™¤æ–œä½“ (*text* æˆ– _text_)
        text = re.sub(r'\*(.+?)\*', r'\1', text)
        text = re.sub(r'_(.+?)_', r'\1', text)

        # ç§»é™¤åˆ é™¤çº¿ (~~text~~)
        text = re.sub(r'~~(.+?)~~', r'\1', text)

        # ç§»é™¤ä»£ç å—æ ‡è®° (```)
        text = re.sub(r'```[\s\S]*?```', '', text)
        text = re.sub(r'`(.+?)`', r'\1', text)

        # ç§»é™¤é“¾æ¥ [text](url)
        text = re.sub(r'\[(.+?)\]\(.+?\)', r'\1', text)

        # ç§»é™¤åˆ—è¡¨æ ‡è®° (-, *, +, 1.)
        text = re.sub(r'^\s*[-*+]\s+', '', text, flags=re.MULTILINE)
        text = re.sub(r'^\s*\d+\.\s+', '', text, flags=re.MULTILINE)

        # ç§»é™¤è¡¨æ ¼åˆ†éš”çº¿ (|---|---|)
        text = re.sub(r'\|[-:\s|]+\|', '', text)

        # ç®€åŒ–è¡¨æ ¼æ ¼å¼ (| cell |) -> cell
        text = re.sub(r'\s*\|\s*', ' ', text)

        # ç§»é™¤å¼•ç”¨æ ‡è®° (>)
        text = re.sub(r'^>\s+', '', text, flags=re.MULTILINE)

        # ç§»é™¤å¤šä½™ç©ºè¡Œ
        text = re.sub(r'\n\n+', '\n\n', text)

        return text.strip()

    @staticmethod
    def format_text(text, max_width=60):
        """å°†markdownæ–‡æœ¬è¿‡æ»¤å¹¶æ™ºèƒ½åˆ†è¡Œï¼Œç”¨äºæ§åˆ¶å°æ˜¾ç¤º"""
        # å…ˆè¿‡æ»¤markdownæ ¼å¼
        text = AIAnalyzer.strip_markdown(text)

        lines = []
        # å…ˆå»æ‰å¤šä½™çš„ç©ºè¡Œï¼Œåˆå¹¶æˆä¸€æ®µ
        text = " ".join(line.strip() for line in text.split("\n") if line.strip())

        # æŒ‰å¥å­åˆ†å‰²ï¼ˆå¥å·ã€é—®å·ã€æ„Ÿå¹å·ã€åˆ†å·ï¼‰
        current_line = ""
        for char in text:
            current_line += char
            # é‡åˆ°å¥å­ç»“æŸç¬¦å·ä¸”é•¿åº¦è¶…è¿‡30å­—ç¬¦ï¼Œæˆ–é•¿åº¦è¶…è¿‡max_width
            if (char in "ã€‚ï¼ï¼Ÿï¼›" and len(current_line) > 30) or len(current_line) >= max_width:
                lines.append(current_line.strip())
                current_line = ""

        # æ·»åŠ å‰©ä½™å†…å®¹
        if current_line.strip():
            lines.append(current_line.strip())

        return lines

    def analyze(self, data_collector, report_dir="reports"):
        """
        æ‰§è¡ŒAIåˆ†æ

        Args:
            data_collector: æ•°æ®æ”¶é›†å™¨å¯¹è±¡ï¼Œéœ€è¦æä¾›ä»¥ä¸‹æ–¹æ³•ï¼š
                - get_market_info(is_return=True)
                - kx(is_return=True)
                - gold(is_return=True)
                - real_time_gold(is_return=True)
                - seven_A(is_return=True)
                - A(is_return=True)
                - bk(is_return=True)
                ä»¥åŠ self.result å’Œ self.CACHE_MAP å±æ€§
            report_dir: AIåˆ†ææŠ¥å‘Šè¾“å‡ºç›®å½•ï¼Œé»˜è®¤ä¸º"reports"
        """
        try:
            from langchain_core.prompts import ChatPromptTemplate
            from langchain_core.output_parsers import StrOutputParser

            logger.debug("æ­£åœ¨æ”¶é›†æ•°æ®è¿›è¡ŒAIåˆ†æ...")

            # åˆå§‹åŒ–LLM
            llm = self.init_langchain_llm()
            if llm is None:
                return

            # æ”¶é›†å¸‚åœºæ•°æ®
            market_data = data_collector.get_market_info(is_return=True)
            market_summary = "ä¸»è¦å¸‚åœºæŒ‡æ•°ï¼š\n"
            for item in market_data[:10]:
                market_summary += f"- {item[0]}: {item[1]} ({item[2]})\n"

            # æ”¶é›†7x24å¿«è®¯
            kx_data = data_collector.kx(is_return=True)
            kx_summary = "7Ã—24å¿«è®¯ï¼ˆæœ€æ–°10æ¡ï¼‰ï¼š\n"
            for i, v in enumerate(kx_data[:10], 1):
                evaluate = v.get("evaluate", "")
                evaluate_tag = f"ã€{evaluate}ã€‘" if evaluate else ""
                title = v.get("title", v.get("content", {}).get("items", [{}])[0].get("data", ""))
                publish_time = datetime.datetime.fromtimestamp(int(v["publish_time"])).strftime("%Y-%m-%d %H:%M:%S")
                entity = v.get("entity", [])
                if entity:
                    entity_str = ", ".join([f"{x['code']}-{x['name']}" for x in entity[:3]])  # æœ€å¤šæ˜¾ç¤º3åªè‚¡ç¥¨
                    kx_summary += f"{i}. {publish_time} {evaluate_tag}{title} (å½±å“: {entity_str})\n"
                else:
                    kx_summary += f"{i}. {publish_time} {evaluate_tag}{title}\n"

            # æ”¶é›†é‡‘ä»·æ•°æ®
            gold_data = data_collector.gold(is_return=True)
            gold_summary = "è¿‘æœŸé‡‘ä»·ï¼ˆæœ€è¿‘5å¤©ï¼‰ï¼š\n"
            for item in gold_data[:5]:
                gold_summary += f"- {item[0]}: ä¸­å›½é»„é‡‘{item[1]}, å‘¨å¤§ç¦{item[2]}, æ¶¨è·Œ({item[3]}, {item[4]})\n"

            # æ”¶é›†å®æ—¶é‡‘ä»·
            realtime_gold_data = data_collector.real_time_gold(is_return=True)
            realtime_gold_summary = "å®æ—¶é‡‘ä»·ï¼š\n"
            if realtime_gold_data and len(realtime_gold_data) == 2:
                for row in realtime_gold_data:
                    if row:
                        realtime_gold_summary += f"- {row[0]}: æœ€æ–°ä»·{row[1]}, æ¶¨è·Œå¹…{row[3]}\n"

            # æ”¶é›†è¿‘7æ—¥æˆäº¤é‡
            seven_a_data = data_collector.seven_A(is_return=True)
            seven_a_summary = "è¿‘7æ—¥æˆäº¤é‡ï¼ˆæœ€è¿‘3å¤©ï¼‰ï¼š\n"
            for item in seven_a_data[:3]:
                seven_a_summary += f"- {item[0]}: æ€»æˆäº¤{item[1]}, ä¸Šäº¤æ‰€{item[2]}, æ·±äº¤æ‰€{item[3]}, åŒ—äº¤æ‰€{item[4]}\n"

            # æ”¶é›†è¿‘30åˆ†é’Ÿä¸Šè¯æŒ‡æ•°
            a_data = data_collector.A(is_return=True)
            a_summary = "è¿‘30åˆ†é’Ÿä¸Šè¯æŒ‡æ•°ï¼ˆæœ€è¿‘5åˆ†é’Ÿï¼‰ï¼š\n"
            for item in a_data[-5:]:
                a_summary += f"- {item[0]}: {item[1]}, æ¶¨è·Œé¢{item[2]}, æ¶¨è·Œå¹…{item[3]}, æˆäº¤é‡{item[4]}, æˆäº¤é¢{item[5]}\n"

            # æ”¶é›†æ¿å—æ•°æ®
            bk_data = data_collector.bk(is_return=True)
            top_sectors = "æ¶¨å¹…å‰5æ¿å—ï¼š\n"
            for i, item in enumerate(bk_data[:5]):
                top_sectors += f"{i+1}. {item[0]}: {item[1]}, ä¸»åŠ›å‡€æµå…¥{item[2]}, ä¸»åŠ›æµå…¥å æ¯”{item[3]}\n"

            bottom_sectors = "è·Œå¹…å5æ¿å—ï¼š\n"
            for i, item in enumerate(bk_data[-5:]):
                bottom_sectors += f"{i+1}. {item[0]}: {item[1]}, ä¸»åŠ›å‡€æµå…¥{item[2]}, ä¸»åŠ›æµå…¥å æ¯”{item[3]}\n"

            # æ”¶é›†åŸºé‡‘æ•°æ®
            fund_data = []
            for fund_code, fund_info in data_collector.CACHE_MAP.items():
                for fund in data_collector.result:
                    if fund[0] == fund_code:
                        fund_data.append({
                            "code": fund[0],
                            "name": AIAnalyzer.clean_ansi_codes(fund[1].replace("â­ ", "")),
                            "forecast": AIAnalyzer.clean_ansi_codes(fund[3]),
                            "growth": AIAnalyzer.clean_ansi_codes(fund[4]),
                            "consecutive": AIAnalyzer.clean_ansi_codes(fund[5]),
                            "consecutive_growth": AIAnalyzer.clean_ansi_codes(fund[6]),
                            "month_stats": AIAnalyzer.clean_ansi_codes(fund[7]),
                            "month_growth": AIAnalyzer.clean_ansi_codes(fund[8]),
                            "is_hold": fund_info.get("is_hold", False)
                        })
                        break

            # æ„å»ºåŸºé‡‘æ‘˜è¦
            fund_summary = f"è‡ªé€‰åŸºé‡‘æ€»æ•°: {len(fund_data)}åª\n\n"

            # æŒæœ‰åŸºé‡‘
            hold_funds = [f for f in fund_data if f["is_hold"]]
            if hold_funds:
                fund_summary += "æŒæœ‰åŸºé‡‘ï¼š\n"
                for i, f in enumerate(hold_funds, 1):
                    fund_summary += f"{i}. {f['name']}: ä¼°å€¼{f['forecast']}, æ—¥æ¶¨å¹…{f['growth']}, è¿ç»­{f['consecutive']}å¤©, è¿‘30å¤©{f['month_stats']}\n"
                fund_summary += "\n"

            # è¡¨ç°æœ€å¥½çš„åŸºé‡‘
            top_funds = sorted(fund_data, key=lambda x: float(x["forecast"].replace("%", "")) if x["forecast"] != "N/A" else -999, reverse=True)[:5]
            fund_summary += "ä»Šæ—¥æ¶¨å¹…å‰5çš„åŸºé‡‘ï¼š\n"
            for i, f in enumerate(top_funds, 1):
                hold_mark = "ã€æŒæœ‰ã€‘" if f["is_hold"] else ""
                fund_summary += f"{i}. {hold_mark}{f['name']}: ä¼°å€¼{f['forecast']}, æ—¥æ¶¨å¹…{f['growth']}\n"

            # åˆ›å»ºæç¤ºé“¾ - å¸‚åœºè¶‹åŠ¿åˆ†æ
            trend_prompt = ChatPromptTemplate.from_messages([
                ("system", "ä½ æ˜¯ä¸€ä½èµ„æ·±é‡‘èåˆ†æå¸ˆï¼Œæ“…é•¿å®è§‚å¸‚åœºåˆ†æå’Œè¶‹åŠ¿åˆ¤æ–­ã€‚è¯·ä»ä¸“ä¸šè§’åº¦æ·±å…¥åˆ†æå¸‚åœºèµ°åŠ¿ã€‚"),
                ("user", """è¯·åŸºäºä»¥ä¸‹å®Œæ•´çš„å¸‚åœºæ•°æ®ï¼Œè¿›è¡Œæ·±å…¥çš„å¸‚åœºè¶‹åŠ¿åˆ†æï¼š

ã€7Ã—24å¿«è®¯ã€‘
{kx_summary}

ã€å¸‚åœºæŒ‡æ•°ã€‘
{market_summary}

ã€é‡‘ä»·èµ°åŠ¿ã€‘
{gold_summary}

{realtime_gold_summary}

ã€å¸‚åœºæˆäº¤é‡ã€‘
{seven_a_summary}

ã€ä¸Šè¯åˆ†æ—¶æ•°æ®ã€‘
{a_summary}

ã€é¢†æ¶¨æ¿å—ã€‘
{top_sectors}

è¯·ä»ä»¥ä¸‹è§’åº¦è¿›è¡Œåˆ†æï¼ˆè¾“å‡º300-400å­—ï¼‰ï¼š
1. ç»“åˆ7Ã—24å¿«è®¯ï¼Œåˆ†æå½“å‰å¸‚åœºçƒ­ç‚¹å’Œé‡è¦äº‹ä»¶
2. åˆ†æä¸»è¦æŒ‡æ•°çš„èµ°åŠ¿ç‰¹å¾å’Œç›¸äº’å…³ç³»
3. åˆ¤æ–­å½“å‰å¸‚åœºæ‰€å¤„çš„é˜¶æ®µï¼ˆä¸Šæ¶¨/éœ‡è¡/è°ƒæ•´ï¼‰
4. åˆ†æå¸‚åœºæƒ…ç»ªå’Œèµ„é‡‘æµå‘ç‰¹å¾ï¼ˆç»“åˆæˆäº¤é‡å’Œåˆ†æ—¶æ•°æ®ï¼‰
5. å¯¹æ¯”å›½å†…å¤–å¸‚åœºè¡¨ç°ï¼ŒæŒ‡å‡ºå…³é”®å½±å“å› ç´ 
6. åˆ†æé‡‘ä»·èµ°åŠ¿å¯¹å¸‚åœºçš„å½±å“

è¯·ç”¨ä¸“ä¸šã€å®¢è§‚çš„è¯­è¨€è¾“å‡ºï¼Œä½¿ç”¨markdownæ ¼å¼ï¼ˆå¯ä½¿ç”¨##ã€###æ ‡é¢˜ï¼Œ**åŠ ç²—**ï¼Œåˆ—è¡¨ï¼Œè¡¨æ ¼ç­‰ï¼‰ï¼Œè¾“å‡ºç»“æ„åŒ–ã€æ˜“è¯»çš„ä¸“ä¸šåˆ†ææŠ¥å‘Šã€‚""")
            ])

            # åˆ›å»ºæç¤ºé“¾ - æ¿å—æœºä¼šåˆ†æ
            sector_prompt = ChatPromptTemplate.from_messages([
                ("system", "ä½ æ˜¯ä¸€ä½è¡Œä¸šç ”ç©¶ä¸“å®¶ï¼Œç²¾é€šå„ä¸ªè¡Œä¸šæ¿å—çš„æŠ•èµ„é€»è¾‘å’Œå‘¨æœŸè§„å¾‹ã€‚"),
                ("user", """è¯·åŸºäºä»¥ä¸‹æ¿å—æ•°æ®å’Œå¸‚åœºç¯å¢ƒï¼Œæ·±å…¥åˆ†æè¡Œä¸šæŠ•èµ„æœºä¼šï¼š

ã€æ¶¨å¹…é¢†å…ˆæ¿å—ã€‘
{top_sectors}

ã€è·Œå¹…æ¿å—ã€‘
{bottom_sectors}

ã€å¸‚åœºæˆäº¤é‡ã€‘
{seven_a_summary}

ã€ä¸Šè¯åˆ†æ—¶ã€‘
{a_summary}

è¯·ä»ä»¥ä¸‹è§’åº¦è¿›è¡Œåˆ†æï¼ˆè¾“å‡º300-400å­—ï¼‰ï¼š
1. åˆ†æé¢†æ¶¨æ¿å—çš„å…±åŒç‰¹å¾å’Œé©±åŠ¨å› ç´ 
2. åˆ¤æ–­è¿™äº›æ¿å—çš„è¡Œæƒ…å¯æŒç»­æ€§ï¼ˆç»“åˆæˆäº¤é‡å’Œèµ„é‡‘æµå‘ï¼‰
3. ç»“åˆèµ„é‡‘æµå…¥æƒ…å†µï¼Œè¯„ä¼°æ¿å—å¼ºåº¦
4. æç¤ºå“ªäº›æ¿å—å€¼å¾—é‡ç‚¹å…³æ³¨ï¼Œç»™å‡ºé…ç½®å»ºè®®
5. åˆ†æå¼±åŠ¿æ¿å—æ˜¯å¦å­˜åœ¨åè½¬æœºä¼š

è¯·ç”¨ä¸“ä¸šã€æ·±å…¥çš„è¯­è¨€è¾“å‡ºï¼Œä½¿ç”¨markdownæ ¼å¼ï¼ˆå¯ä½¿ç”¨##ã€###æ ‡é¢˜ï¼Œ**åŠ ç²—**ï¼Œåˆ—è¡¨ï¼Œè¡¨æ ¼ç­‰ï¼‰ï¼Œè¾“å‡ºç»“æ„åŒ–ã€æ˜“è¯»çš„ä¸“ä¸šåˆ†ææŠ¥å‘Šã€‚""")
            ])

            # åˆ›å»ºæç¤ºé“¾ - åŸºé‡‘ç»„åˆå»ºè®®
            portfolio_prompt = ChatPromptTemplate.from_messages([
                ("system", "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„åŸºé‡‘æŠ•èµ„é¡¾é—®ï¼Œæ“…é•¿åŸºé‡‘ç»„åˆé…ç½®å’Œé£é™©ç®¡ç†ã€‚"),
                ("user", """è¯·åŸºäºä»¥ä¸‹åŸºé‡‘æŒä»“å’Œå®Œæ•´å¸‚åœºç¯å¢ƒï¼Œç»™å‡ºæŠ•èµ„å»ºè®®ï¼š

ã€åŸºé‡‘æŒä»“ã€‘
{fund_summary}

ã€å¸‚åœºç¯å¢ƒã€‘
{market_summary}

ã€å¸‚åœºæˆäº¤é‡ã€‘
{seven_a_summary}

ã€æ¿å—è¡¨ç°ã€‘
{top_sectors}

è¯·ä»ä»¥ä¸‹è§’åº¦ç»™å‡ºå»ºè®®ï¼ˆè¾“å‡º300-400å­—ï¼‰ï¼š
1. è¯„ä¼°å½“å‰æŒä»“åŸºé‡‘çš„è¡¨ç°å’Œé£é™©ç‰¹å¾
2. åˆ†ææŒä»“åŸºé‡‘ä¸å¸‚åœºç¯å¢ƒçš„åŒ¹é…åº¦ï¼ˆç»“åˆæˆäº¤é‡å’Œæ¿å—è½®åŠ¨ï¼‰
3. ç»™å‡ºå…·ä½“çš„è°ƒä»“å»ºè®®ï¼ˆå¢æŒ/å‡æŒ/æŒæœ‰ï¼‰
4. å¯¹è¡¨ç°ä¼˜å¼‚çš„åŸºé‡‘ï¼Œåˆ†æèƒŒååŸå› å’Œå¯æŒç»­æ€§
5. æç¤ºä»“ä½é…ç½®å’Œé£é™©æ•å£çš„ä¼˜åŒ–æ–¹å‘

è¯·ç»™å‡ºå…·ä½“ã€å¯æ“ä½œçš„å»ºè®®ï¼Œä½¿ç”¨markdownæ ¼å¼ï¼ˆå¯ä½¿ç”¨##ã€###æ ‡é¢˜ï¼Œ**åŠ ç²—**ï¼Œåˆ—è¡¨ï¼Œè¡¨æ ¼ç­‰ï¼‰ï¼Œè¾“å‡ºç»“æ„åŒ–ã€æ˜“è¯»çš„ä¸“ä¸šåˆ†ææŠ¥å‘Šã€‚""")
            ])

            # åˆ›å»ºæç¤ºé“¾ - é£é™©æç¤º
            risk_prompt = ChatPromptTemplate.from_messages([
                ("system", "ä½ æ˜¯ä¸€ä½é£é™©ç®¡ç†ä¸“å®¶ï¼Œæ“…é•¿è¯†åˆ«å¸‚åœºé£é™©å’Œåˆ¶å®šé£æ§ç­–ç•¥ã€‚"),
                ("user", """è¯·åŸºäºå½“å‰å®Œæ•´çš„å¸‚åœºæ•°æ®ï¼Œè¿›è¡Œå…¨é¢çš„é£é™©åˆ†æï¼š

ã€å¸‚åœºæŒ‡æ•°ã€‘
{market_summary}

ã€é‡‘ä»·èµ°åŠ¿ã€‘
{gold_summary}

ã€å¸‚åœºæˆäº¤é‡ã€‘
{seven_a_summary}

ã€ä¸Šè¯åˆ†æ—¶ã€‘
{a_summary}

ã€æ¿å—è¡¨ç°ã€‘
{top_sectors}
{bottom_sectors}

ã€åŸºé‡‘æŒä»“ã€‘
{fund_summary}

è¯·ä»ä»¥ä¸‹è§’åº¦è¿›è¡Œé£é™©åˆ†æï¼ˆè¾“å‡º250-350å­—ï¼‰ï¼š
1. è¯†åˆ«å½“å‰å¸‚åœºçš„ä¸»è¦é£é™©ç‚¹ï¼ˆç»“åˆæˆäº¤é‡èç¼©/æ”¾å¤§ã€åˆ†æ—¶èµ°åŠ¿ç­‰ï¼‰
2. åˆ†æå¯èƒ½å¼•å‘è°ƒæ•´çš„è§¦å‘å› ç´ 
3. è¯„ä¼°æŒä»“åŸºé‡‘çš„é£é™©æš´éœ²
4. ç»™å‡ºé£é™©é˜²æ§å»ºè®®å’Œåº”å¯¹ç­–ç•¥
5. æç¤ºéœ€è¦å…³æ³¨çš„é£é™©ä¿¡å·ï¼ˆåŒ…æ‹¬æŠ€æœ¯é¢å’Œèµ„é‡‘é¢ï¼‰

è¯·å®¢è§‚ã€è°¨æ…åœ°æç¤ºé£é™©ï¼Œä½¿ç”¨markdownæ ¼å¼ï¼ˆå¯ä½¿ç”¨##ã€###æ ‡é¢˜ï¼Œ**åŠ ç²—**ï¼Œåˆ—è¡¨ï¼Œè¡¨æ ¼ç­‰ï¼‰ï¼Œè¾“å‡ºç»“æ„åŒ–ã€æ˜“è¯»çš„ä¸“ä¸šåˆ†ææŠ¥å‘Šã€‚""")
            ])

            # åˆ›å»ºè¾“å‡ºè§£æå™¨
            output_parser = StrOutputParser()

            # æ‰§è¡Œå››ä¸ªç»´åº¦çš„åˆ†æ
            logger.info("æ­£åœ¨è¿›è¡Œå¸‚åœºè¶‹åŠ¿åˆ†æ...")
            trend_chain = trend_prompt | llm | output_parser
            trend_analysis = trend_chain.invoke({
                "kx_summary": kx_summary,
                "market_summary": market_summary,
                "gold_summary": gold_summary,
                "realtime_gold_summary": realtime_gold_summary,
                "seven_a_summary": seven_a_summary,
                "a_summary": a_summary,
                "top_sectors": top_sectors
            })

            logger.info("æ­£åœ¨è¿›è¡Œæ¿å—æœºä¼šåˆ†æ...")
            sector_chain = sector_prompt | llm | output_parser
            sector_analysis = sector_chain.invoke({
                "top_sectors": top_sectors,
                "bottom_sectors": bottom_sectors,
                "seven_a_summary": seven_a_summary,
                "a_summary": a_summary
            })

            logger.info("æ­£åœ¨è¿›è¡ŒåŸºé‡‘ç»„åˆåˆ†æ...")
            portfolio_chain = portfolio_prompt | llm | output_parser
            portfolio_analysis = portfolio_chain.invoke({
                "fund_summary": fund_summary,
                "market_summary": market_summary,
                "seven_a_summary": seven_a_summary,
                "top_sectors": top_sectors
            })

            logger.info("æ­£åœ¨è¿›è¡Œé£é™©åˆ†æ...")
            risk_chain = risk_prompt | llm | output_parser
            risk_analysis = risk_chain.invoke({
                "market_summary": market_summary,
                "gold_summary": gold_summary,
                "seven_a_summary": seven_a_summary,
                "a_summary": a_summary,
                "top_sectors": top_sectors,
                "bottom_sectors": bottom_sectors,
                "fund_summary": fund_summary
            })

            # ç”Ÿæˆmarkdownæ–‡ä»¶å†…å®¹
            markdown_content = f"""# AIå¸‚åœºæ·±åº¦åˆ†ææŠ¥å‘Š

**ç”Ÿæˆæ—¶é—´**ï¼š{time.strftime('%Y-%m-%d %H:%M')}

---

## ğŸ“Š åŸå§‹æ•°æ®æ¦‚è§ˆ

### 7Ã—24å¿«è®¯

{kx_summary}

### å¸‚åœºæŒ‡æ•°

{market_summary}

### é‡‘ä»·èµ°åŠ¿

{gold_summary}

{realtime_gold_summary}

### å¸‚åœºæˆäº¤é‡

{seven_a_summary}

### ä¸Šè¯æŒ‡æ•°åˆ†æ—¶ï¼ˆæœ€è¿‘5åˆ†é’Ÿï¼‰

{a_summary}

### æ¶¨å¹…é¢†å…ˆæ¿å—ï¼ˆTop 5ï¼‰

{top_sectors}

### è·Œå¹…æ¿å—ï¼ˆBottom 5ï¼‰

{bottom_sectors}

### åŸºé‡‘æŒä»“æƒ…å†µ

{fund_summary}

---

## 1ï¸âƒ£ å¸‚åœºæ•´ä½“è¶‹åŠ¿åˆ†æ

{trend_analysis}

---

## 2ï¸âƒ£ è¡Œä¸šæ¿å—æœºä¼šåˆ†æ

{sector_analysis}

---

## 3ï¸âƒ£ åŸºé‡‘ç»„åˆæŠ•èµ„å»ºè®®

{portfolio_analysis}

---

## 4ï¸âƒ£ é£é™©æç¤ºä¸åº”å¯¹

{risk_analysis}

---

ğŸ’¡ **æç¤º**ï¼šä»¥ä¸Šåˆ†æç”±AIç”Ÿæˆï¼Œä»…ä¾›å‚è€ƒï¼Œä¸æ„æˆæŠ•èµ„å»ºè®®ã€‚æŠ•èµ„æœ‰é£é™©ï¼Œå…¥å¸‚éœ€è°¨æ…ã€‚
"""

            # ä¿å­˜markdownæ–‡ä»¶
            if not os.path.exists(report_dir):
                os.makedirs(report_dir, exist_ok=True)

            report_filename = f"{report_dir}/AIå¸‚åœºåˆ†ææŠ¥å‘Š{time.strftime('%Y%m%d_%H%M%S')}.md"
            with open(report_filename, "w", encoding="utf-8") as f:
                f.write(markdown_content)

            logger.info(f"âœ… AIåˆ†ææŠ¥å‘Šå·²ä¿å­˜è‡³ï¼š{report_filename}")

            # è¾“å‡ºå®Œæ•´çš„AIåˆ†ææŠ¥å‘Š
            logger.critical(f"{time.strftime('%Y-%m-%d %H:%M')} ğŸ“Š AIå¸‚åœºæ·±åº¦åˆ†ææŠ¥å‘Š")
            logger.info("=" * 80)

            logger.info("1ï¸âƒ£ å¸‚åœºæ•´ä½“è¶‹åŠ¿åˆ†æ")
            logger.info("-" * 80)
            for line in self.format_text(trend_analysis):
                logger.info(line)

            logger.info("=" * 80)
            logger.info("2ï¸âƒ£ è¡Œä¸šæ¿å—æœºä¼šåˆ†æ")
            logger.info("-" * 80)
            for line in self.format_text(sector_analysis):
                logger.info(line)

            logger.info("=" * 80)
            logger.info("3ï¸âƒ£ åŸºé‡‘ç»„åˆæŠ•èµ„å»ºè®®")
            logger.info("-" * 80)
            for line in self.format_text(portfolio_analysis):
                logger.info(line)

            logger.info("=" * 80)
            logger.info("4ï¸âƒ£ é£é™©æç¤ºä¸åº”å¯¹")
            logger.info("-" * 80)
            for line in self.format_text(risk_analysis):
                logger.info(line)

            logger.info("=" * 80)
            logger.info("ğŸ’¡ æç¤ºï¼šä»¥ä¸Šåˆ†æç”±AIç”Ÿæˆï¼Œä»…ä¾›å‚è€ƒï¼Œä¸æ„æˆæŠ•èµ„å»ºè®®ã€‚æŠ•èµ„æœ‰é£é™©ï¼Œå…¥å¸‚éœ€è°¨æ…ã€‚")
            logger.info("=" * 80)

        except Exception as e:
            logger.error(f"AIåˆ†æè¿‡ç¨‹å‡ºé”™: {e}")
            import traceback
            logger.error(traceback.format_exc())

    def analyze_fast(self, data_collector, report_dir="reports"):
        """
        å¿«é€Ÿåˆ†ææ¨¡å¼ - ä¸€æ¬¡æ€§ç”Ÿæˆç®€æ˜åˆ†ææŠ¥å‘Š

        Args:
            data_collector: æ•°æ®æ”¶é›†å™¨å¯¹è±¡
            report_dir: AIåˆ†ææŠ¥å‘Šè¾“å‡ºç›®å½•ï¼Œé»˜è®¤ä¸º"reports"
        """
        try:
            from langchain_core.prompts import ChatPromptTemplate
            from langchain_core.output_parsers import StrOutputParser

            logger.info("ğŸš€ å¯åŠ¨å¿«é€Ÿåˆ†ææ¨¡å¼...")

            # åˆå§‹åŒ–LLMï¼ˆå¿«é€Ÿæ¨¡å¼ï¼‰
            llm = self.init_langchain_llm(fast_mode=True)
            if llm is None:
                return

            # æ”¶é›†å¸‚åœºæ•°æ®
            market_data = data_collector.get_market_info(is_return=True)
            market_summary = "ä¸»è¦å¸‚åœºæŒ‡æ•°ï¼š\n"
            for item in market_data[:8]:
                market_summary += f"- {item[0]}: {item[1]} ({item[2]})\n"

            # æ”¶é›†7x24å¿«è®¯ï¼ˆåªå–å‰5æ¡ï¼‰
            kx_data = data_collector.kx(is_return=True)
            kx_summary = "æœ€æ–°å¿«è®¯ï¼ˆå‰5æ¡ï¼‰ï¼š\n"
            for i, v in enumerate(kx_data[:5], 1):
                evaluate = v.get("evaluate", "")
                evaluate_tag = f"ã€{evaluate}ã€‘" if evaluate else ""
                title = v.get("title", v.get("content", {}).get("items", [{}])[0].get("data", ""))
                kx_summary += f"{i}. {evaluate_tag}{title}\n"

            # æ”¶é›†æ¿å—æ•°æ®
            bk_data = data_collector.bk(is_return=True)
            top_sectors = "æ¶¨å¹…å‰5æ¿å—ï¼š\n"
            for i, item in enumerate(bk_data[:5]):
                top_sectors += f"{i+1}. {item[0]}: {item[1]}, ä¸»åŠ›å‡€æµå…¥{item[2]}\n"

            # æ”¶é›†åŸºé‡‘æ•°æ®
            fund_data = []
            for fund_code, fund_info in data_collector.CACHE_MAP.items():
                for fund in data_collector.result:
                    if fund[0] == fund_code:
                        fund_data.append({
                            "code": fund[0],
                            "name": AIAnalyzer.clean_ansi_codes(fund[1].replace("â­ ", "")),
                            "forecast": AIAnalyzer.clean_ansi_codes(fund[3]),
                            "growth": AIAnalyzer.clean_ansi_codes(fund[4]),
                            "is_hold": fund_info.get("is_hold", False)
                        })
                        break

            # æ„å»ºåŸºé‡‘æ‘˜è¦
            fund_summary = f"è‡ªé€‰åŸºé‡‘æ€»æ•°: {len(fund_data)}åª\n"
            hold_funds = [f for f in fund_data if f["is_hold"]]
            if hold_funds:
                fund_summary += f"æŒæœ‰åŸºé‡‘æ•°: {len(hold_funds)}åª\n"

            # è¡¨ç°æœ€å¥½çš„åŸºé‡‘
            top_funds = sorted(fund_data, key=lambda x: float(x["forecast"].replace("%", "")) if x["forecast"] != "N/A" else -999, reverse=True)[:3]
            fund_summary += "ä»Šæ—¥æ¶¨å¹…å‰3çš„åŸºé‡‘ï¼š\n"
            for i, f in enumerate(top_funds, 1):
                hold_mark = "ã€æŒæœ‰ã€‘" if f["is_hold"] else ""
                fund_summary += f"{i}. {hold_mark}{f['name']}: ä¼°å€¼{f['forecast']}\n"

            # åˆ›å»ºä¸€æ¬¡æ€§åˆ†ææç¤º
            fast_prompt = ChatPromptTemplate.from_messages([
                ("system", "ä½ æ˜¯ä¸€ä½èµ„æ·±é‡‘èåˆ†æå¸ˆï¼Œæ“…é•¿å¿«é€ŸæŠ“ä½å¸‚åœºè¦ç‚¹ã€‚"),
                ("user", """è¯·åŸºäºä»¥ä¸‹å¸‚åœºæ•°æ®ï¼Œç”Ÿæˆç®€æ˜æ‰¼è¦çš„å¸‚åœºåˆ†ææŠ¥å‘Šï¼š

ã€7Ã—24å¿«è®¯ã€‘
{kx_summary}

ã€å¸‚åœºæŒ‡æ•°ã€‘
{market_summary}

ã€é¢†æ¶¨æ¿å—ã€‘
{top_sectors}

ã€åŸºé‡‘æŒä»“ã€‘
{fund_summary}

è¯·ç”Ÿæˆä¸€ä»½ç®€æ˜çš„å¸‚åœºåˆ†ææŠ¥å‘Šï¼ŒåŒ…å«ä»¥ä¸‹4ä¸ªéƒ¨åˆ†ï¼ˆæ€»å…±400-500å­—ï¼‰ï¼š

## 1. å¸‚åœºè¶‹åŠ¿ï¼ˆ100å­—ï¼‰
ç®€è¦åˆ†æå½“å‰å¸‚åœºçƒ­ç‚¹ã€æ•´ä½“èµ°åŠ¿å’Œå¸‚åœºæƒ…ç»ªã€‚

## 2. æ¿å—æœºä¼šï¼ˆ80å­—ï¼‰
æŒ‡å‡ºé¢†æ¶¨æ¿å—çš„ç‰¹å¾å’Œå€¼å¾—å…³æ³¨çš„æŠ•èµ„æœºä¼šã€‚

## 3. åŸºé‡‘å»ºè®®ï¼ˆ80å­—ï¼‰
è¯„ä¼°æŒä»“åŸºé‡‘è¡¨ç°ï¼Œç»™å‡ºç®€è¦çš„é…ç½®å»ºè®®ã€‚

## 4. é£é™©æç¤ºï¼ˆ80å­—ï¼‰
æç¤ºå½“å‰ä¸»è¦é£é™©ç‚¹å’Œåº”å¯¹ç­–ç•¥ã€‚

è¾“å‡ºè¦æ±‚ï¼šä½¿ç”¨markdownæ ¼å¼ï¼Œç®€æ´æ˜äº†ï¼Œè¦ç‚¹çªå‡ºã€‚""")
            ])

            # æ‰§è¡Œå¿«é€Ÿåˆ†æ
            logger.info("æ­£åœ¨ç”Ÿæˆå¿«é€Ÿåˆ†ææŠ¥å‘Š...")
            output_parser = StrOutputParser()
            fast_chain = fast_prompt | llm | output_parser

            analysis_result = fast_chain.invoke({
                "kx_summary": kx_summary,
                "market_summary": market_summary,
                "top_sectors": top_sectors,
                "fund_summary": fund_summary
            })

            # ç”ŸæˆmarkdownæŠ¥å‘Š
            markdown_content = f"""# ğŸ“Š AIå¿«é€Ÿå¸‚åœºåˆ†ææŠ¥å‘Š

**ç”Ÿæˆæ—¶é—´**: {time.strftime('%Y-%m-%d %H:%M:%S')}

---

{analysis_result}

---

ğŸ’¡ **æç¤º**ï¼šå¿«é€Ÿåˆ†ææ¨¡å¼ï¼Œä»…ä¾›å‚è€ƒï¼Œä¸æ„æˆæŠ•èµ„å»ºè®®ã€‚æŠ•èµ„æœ‰é£é™©ï¼Œå…¥å¸‚éœ€è°¨æ…ã€‚
"""

            # ä¿å­˜markdownæ–‡ä»¶
            if not os.path.exists(report_dir):
                os.makedirs(report_dir, exist_ok=True)

            report_filename = f"{report_dir}/AIå¿«é€Ÿåˆ†ææŠ¥å‘Š{time.strftime('%Y%m%d_%H%M%S')}.md"
            with open(report_filename, "w", encoding="utf-8") as f:
                f.write(markdown_content)

            logger.info(f"âœ… å¿«é€Ÿåˆ†ææŠ¥å‘Šå·²ä¿å­˜è‡³ï¼š{report_filename}")

            # è¾“å‡ºåˆ†ææŠ¥å‘Š
            logger.critical(f"{time.strftime('%Y-%m-%d %H:%M')} ğŸ“Š AIå¿«é€Ÿå¸‚åœºåˆ†ææŠ¥å‘Š")
            logger.info("=" * 80)
            for line in self.format_text(analysis_result):
                logger.info(line)
            logger.info("=" * 80)
            logger.info("ğŸ’¡ æç¤ºï¼šå¿«é€Ÿåˆ†ææ¨¡å¼ï¼Œä»…ä¾›å‚è€ƒï¼Œä¸æ„æˆæŠ•èµ„å»ºè®®ã€‚")
            logger.info("=" * 80)

        except Exception as e:
            logger.error(f"å¿«é€Ÿåˆ†æè¿‡ç¨‹å‡ºé”™: {e}")
            import traceback
            logger.error(traceback.format_exc())

    def analyze_deep(self, data_collector, report_dir="reports"):
        """
        æ·±åº¦ç ”ç©¶æ¨¡å¼ - ä½¿ç”¨ ReAct Agent è‡ªä¸»æ”¶é›†æ•°æ®å¹¶ç”ŸæˆæŠ¥å‘Š

        Args:
            data_collector: æ•°æ®æ”¶é›†å™¨å¯¹è±¡
            report_dir: AIåˆ†ææŠ¥å‘Šè¾“å‡ºç›®å½•ï¼Œé»˜è®¤ä¸º"reports"
        """
        try:
            from langchain.agents import create_react_agent, AgentExecutor
            from langchain.tools import tool
            from langchain_core.prompts import PromptTemplate
            from langchain_core.output_parsers import StrOutputParser
            from ddgs import DDGS
            from bs4 import BeautifulSoup
            import requests
            import urllib3
            urllib3.disable_warnings()

            logger.info("ğŸš€ å¯åŠ¨æ·±åº¦ç ”ç©¶æ¨¡å¼...")

            # åˆå§‹åŒ–LLMï¼ˆæ·±åº¦æ¨¡å¼ä½¿ç”¨æ›´é«˜çš„tokené™åˆ¶ï¼Œæ”¯æŒç”Ÿæˆ10,000+å­—é•¿æŠ¥å‘Šï¼‰
            llm = self.init_langchain_llm(fast_mode=False, deep_mode=True)
            if llm is None:
                return

            # å®šä¹‰å·¥å…·å‡½æ•° - åŒ…è£…æ•°æ®æ”¶é›†å™¨çš„æ–¹æ³•ä¸º LangChain tools

            @tool
            def get_market_indices() -> str:
                """è·å–å¸‚åœºæŒ‡æ•°æ•°æ®ï¼ˆä¸Šè¯ã€æ·±è¯ã€çº³æŒ‡ã€é“æŒ‡ç­‰ï¼‰"""
                try:
                    market_data = data_collector.get_market_info(is_return=True)
                    result = "ä¸»è¦å¸‚åœºæŒ‡æ•°ï¼š\n"
                    for item in market_data[:12]:
                        result += f"- {item[0]}: {item[1]} ({item[2]})\n"
                    return result
                except Exception as e:
                    return f"è·å–å¸‚åœºæŒ‡æ•°å¤±è´¥: {str(e)}"

            @tool
            def get_news_flash(count: str = "30") -> str:
                """è·å–7Ã—24å¿«è®¯ï¼ˆå¸‚åœºæ–°é—»ï¼‰

                Args:
                    count: è¦è·å–çš„å¿«è®¯æ•°é‡ï¼Œé»˜è®¤30æ¡ï¼Œæœ€å¤š50æ¡
                """
                try:
                    # è§£æå‚æ•°ï¼ˆæ”¯æŒç›´æ¥ä¼ å…¥æ•°å­—æˆ–JSONå­—ç¬¦ä¸²ï¼‰
                    import json
                    if isinstance(count, str):
                        if count.strip().startswith('{'):
                            # å¦‚æœæ˜¯JSONæ ¼å¼: {"count": 30}
                            try:
                                parsed = json.loads(count)
                                count = parsed.get('count', 30)
                            except:
                                count = 30
                        else:
                            # å¦‚æœæ˜¯çº¯æ•°å­—å­—ç¬¦ä¸²: "30"
                            try:
                                count = int(count)
                            except:
                                count = 30

                    # é™åˆ¶æœ€å¤§æ•°é‡
                    count = min(int(count), 50)
                    kx_data = data_collector.kx(is_return=True, count=count)
                    result = f"7Ã—24å¿«è®¯ï¼ˆæœ€æ–°{len(kx_data)}æ¡ï¼‰ï¼š\n\n"
                    for i, v in enumerate(kx_data[:count], 1):
                        evaluate = v.get("evaluate", "")
                        evaluate_tag = f"ã€{evaluate}ã€‘" if evaluate else ""
                        title = v.get("title", v.get("content", {}).get("items", [{}])[0].get("data", ""))
                        publish_time = datetime.datetime.fromtimestamp(int(v["publish_time"])).strftime("%Y-%m-%d %H:%M:%S")
                        entity = v.get("entity", [])
                        if entity:
                            entity_str = ", ".join([f"{x['code']}-{x['name']}" for x in entity[:3]])
                            result += f"{i}. {publish_time} {evaluate_tag}{title}\n   å½±å“: {entity_str}\n\n"
                        else:
                            result += f"{i}. {publish_time} {evaluate_tag}{title}\n\n"
                    return result
                except Exception as e:
                    return f"è·å–7Ã—24å¿«è®¯å¤±è´¥: {str(e)}"

            @tool
            def get_sector_performance() -> str:
                """è·å–è¡Œä¸šæ¿å—è¡¨ç°ï¼ˆæ¶¨è·Œå¹…ã€èµ„é‡‘æµå‘ç­‰ï¼‰"""
                try:
                    bk_data = data_collector.bk(is_return=True)
                    result = "æ¶¨å¹…å‰10æ¿å—ï¼š\n"
                    for i, item in enumerate(bk_data[:10], 1):
                        result += f"{i}. {item[0]}: {item[1]}, ä¸»åŠ›å‡€æµå…¥{item[2]}, æµå…¥å æ¯”{item[3]}\n"

                    result += "\nè·Œå¹…å10æ¿å—ï¼š\n"
                    for i, item in enumerate(bk_data[-10:], 1):
                        result += f"{i}. {item[0]}: {item[1]}, ä¸»åŠ›å‡€æµå…¥{item[2]}, æµå…¥å æ¯”{item[3]}\n"
                    return result
                except Exception as e:
                    return f"è·å–æ¿å—æ•°æ®å¤±è´¥: {str(e)}"

            @tool
            def get_gold_prices() -> str:
                """è·å–é»„é‡‘ä»·æ ¼æ•°æ®ï¼ˆè¿‘æœŸé‡‘ä»·å’Œå®æ—¶é‡‘ä»·ï¼‰"""
                try:
                    # è¿‘æœŸé‡‘ä»·
                    gold_data = data_collector.gold(is_return=True)
                    result = "è¿‘æœŸé‡‘ä»·ï¼ˆæœ€è¿‘7å¤©ï¼‰ï¼š\n"
                    for item in gold_data[:7]:
                        result += f"- {item[0]}: ä¸­å›½é»„é‡‘{item[1]}, å‘¨å¤§ç¦{item[2]}, æ¶¨è·Œ({item[3]}, {item[4]})\n"

                    # å®æ—¶é‡‘ä»·
                    realtime_gold_data = data_collector.real_time_gold(is_return=True)
                    result += "\nå®æ—¶é‡‘ä»·ï¼š\n"
                    if realtime_gold_data and len(realtime_gold_data) == 2:
                        for row in realtime_gold_data:
                            if row:
                                result += f"- {row[0]}: æœ€æ–°ä»·{row[1]}, æ¶¨è·Œå¹…{row[3]}\n"
                    return result
                except Exception as e:
                    return f"è·å–é‡‘ä»·æ•°æ®å¤±è´¥: {str(e)}"

            @tool
            def get_trading_volume() -> str:
                """è·å–è¿‘7æ—¥å¸‚åœºæˆäº¤é‡æ•°æ®"""
                try:
                    seven_a_data = data_collector.seven_A(is_return=True)
                    result = "è¿‘7æ—¥æˆäº¤é‡ï¼š\n"
                    for item in seven_a_data[:7]:
                        result += f"- {item[0]}: æ€»æˆäº¤{item[1]}, ä¸Šäº¤æ‰€{item[2]}, æ·±äº¤æ‰€{item[3]}, åŒ—äº¤æ‰€{item[4]}\n"
                    return result
                except Exception as e:
                    return f"è·å–æˆäº¤é‡æ•°æ®å¤±è´¥: {str(e)}"

            @tool
            def get_shanghai_intraday() -> str:
                """è·å–ä¸Šè¯æŒ‡æ•°è¿‘30åˆ†é’Ÿåˆ†æ—¶æ•°æ®"""
                try:
                    a_data = data_collector.A(is_return=True)
                    result = "ä¸Šè¯æŒ‡æ•°è¿‘30åˆ†é’Ÿåˆ†æ—¶ï¼ˆæœ€æ–°10åˆ†é’Ÿï¼‰ï¼š\n"
                    for item in a_data[-10:]:
                        result += f"- {item[0]}: {item[1]}, æ¶¨è·Œé¢{item[2]}, æ¶¨è·Œå¹…{item[3]}, æˆäº¤é‡{item[4]}, æˆäº¤é¢{item[5]}\n"
                    return result
                except Exception as e:
                    return f"è·å–ä¸Šè¯åˆ†æ—¶æ•°æ®å¤±è´¥: {str(e)}"

            @tool
            def get_fund_portfolio() -> str:
                """è·å–è‡ªé€‰åŸºé‡‘ç»„åˆçš„è¯¦ç»†æ•°æ®"""
                try:
                    fund_data = []
                    for fund_code, fund_info in data_collector.CACHE_MAP.items():
                        for fund in data_collector.result:
                            if fund[0] == fund_code:
                                fund_data.append({
                                    "code": fund[0],
                                    "name": AIAnalyzer.clean_ansi_codes(fund[1].replace("â­ ", "")),
                                    "forecast": AIAnalyzer.clean_ansi_codes(fund[3]),
                                    "growth": AIAnalyzer.clean_ansi_codes(fund[4]),
                                    "consecutive": AIAnalyzer.clean_ansi_codes(fund[5]),
                                    "consecutive_growth": AIAnalyzer.clean_ansi_codes(fund[6]),
                                    "month_stats": AIAnalyzer.clean_ansi_codes(fund[7]),
                                    "month_growth": AIAnalyzer.clean_ansi_codes(fund[8]),
                                    "is_hold": fund_info.get("is_hold", False)
                                })
                                break

                    result = f"è‡ªé€‰åŸºé‡‘æ€»æ•°: {len(fund_data)}åª\n\n"

                    # æŒæœ‰åŸºé‡‘
                    hold_funds = [f for f in fund_data if f["is_hold"]]
                    if hold_funds:
                        result += f"æŒæœ‰åŸºé‡‘ï¼ˆ{len(hold_funds)}åªï¼‰ï¼š\n"
                        for i, f in enumerate(hold_funds, 1):
                            result += f"{i}. {f['name']}({f['code']}): ä¼°å€¼{f['forecast']}, æ—¥æ¶¨å¹…{f['growth']}, è¿ç»­{f['consecutive']}å¤©, è¿‘30å¤©{f['month_stats']}\n"
                        result += "\n"

                    # è¡¨ç°æœ€å¥½çš„åŸºé‡‘
                    top_funds = sorted(fund_data, key=lambda x: float(x["forecast"].replace("%", "")) if x["forecast"] != "N/A" else -999, reverse=True)[:8]
                    result += "ä»Šæ—¥æ¶¨å¹…å‰8çš„åŸºé‡‘ï¼š\n"
                    for i, f in enumerate(top_funds, 1):
                        hold_mark = "ã€æŒæœ‰ã€‘" if f["is_hold"] else ""
                        result += f"{i}. {hold_mark}{f['name']}({f['code']}): ä¼°å€¼{f['forecast']}, æ—¥æ¶¨å¹…{f['growth']}, è¿‘30å¤©{f['month_stats']}\n"

                    return result
                except Exception as e:
                    return f"è·å–åŸºé‡‘ç»„åˆæ•°æ®å¤±è´¥: {str(e)}"

            @tool
            def fetch_webpage(url: str) -> str:
                """è·å–ç½‘é¡µå†…å®¹å¹¶æå–æ–‡æœ¬

                Args:
                    url: ç½‘é¡µURL
                """
                try:
                    # è§£æå‚æ•°ï¼ˆæ”¯æŒç›´æ¥ä¼ å…¥å­—ç¬¦ä¸²æˆ–JSONå­—ç¬¦ä¸²ï¼‰
                    import json
                    if isinstance(url, str):
                        if url.strip().startswith('{'):
                            # å¦‚æœæ˜¯JSONæ ¼å¼: {"url": "https://..."}
                            try:
                                parsed = json.loads(url)
                                url = parsed.get('url', '')
                            except:
                                pass  # ä¿æŒåŸå§‹url

                    headers = {
                        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
                    }
                    response = requests.get(url, headers=headers, timeout=10, verify=False)
                    response.encoding = response.apparent_encoding

                    soup = BeautifulSoup(response.text, 'lxml')

                    # ç§»é™¤scriptå’Œstyleæ ‡ç­¾
                    for script in soup(["script", "style"]):
                        script.decompose()

                    # æå–æ–‡æœ¬
                    text = soup.get_text()

                    # æ¸…ç†æ–‡æœ¬
                    lines = (line.strip() for line in text.splitlines())
                    chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
                    text = '\n'.join(chunk for chunk in chunks if chunk)

                    # é™åˆ¶é•¿åº¦
                    if len(text) > 3000:
                        text = text[:3000] + "...(å†…å®¹è¿‡é•¿å·²æˆªæ–­)"

                    return f"ç½‘é¡µå†…å®¹ï¼š\n{text}"
                except Exception as e:
                    return f"è·å–ç½‘é¡µå¤±è´¥: {str(e)}"

            @tool
            def get_current_time() -> str:
                """è·å–å½“å‰æ—¥æœŸå’Œæ—¶é—´"""
                return f"å½“å‰æ—¶é—´: {datetime.datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')}"

            # ç»„è£…å·¥å…·åˆ—è¡¨
            tools = [
                get_market_indices,
                get_news_flash,
                get_sector_performance,
                get_gold_prices,
                get_trading_volume,
                get_shanghai_intraday,
                get_fund_portfolio,
                search_news,
                fetch_webpage,
                get_current_time
            ]

            # åˆ›å»ºReAct Agentçš„prompt
            current_date = datetime.datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥')

            react_prompt = PromptTemplate.from_template("""ä½ æ˜¯ä¸€ä½èµ„æ·±é‡‘èç ”ç©¶åˆ†æå¸ˆï¼Œæ“…é•¿æ·±åº¦å¸‚åœºç ”ç©¶å’Œæ•°æ®å¯è§†åŒ–ã€‚ä»Šå¤©æ˜¯{current_date}ã€‚

ä½ çš„ä»»åŠ¡æ˜¯ï¼šé€šè¿‡è‡ªä¸»è°ƒç”¨å·¥å…·æ”¶é›†æ•°æ®ï¼Œç”Ÿæˆä¸€ä»½**æ ¼å¼ä¸°å¯Œã€ç»“æ„æ¸…æ™°ã€æ•°æ®è¯¦å®**çš„å¸‚åœºåˆ†ææŠ¥å‘Šã€‚

**å·¥å…·ä½¿ç”¨è¯´æ˜**ï¼š
- ğŸ“° **get_news_flash**ï¼šè·å–7Ã—24å¿«è®¯åˆ—è¡¨ï¼ˆåŒ…å«æ ‡é¢˜å’Œæ‘˜è¦ï¼‰
- ğŸ” **search_news**ï¼šæ ¹æ®å…³é”®è¯æœç´¢å¿«è®¯çš„è¯¦ç»†å†…å®¹å’Œç›¸å…³æŠ¥é“
- ğŸ“„ **fetch_webpage**ï¼šè·å–å®Œæ•´æ–°é—»æ–‡ç« çš„è¯¦ç»†å†…å®¹
- ğŸ’¡ **å»ºè®®æµç¨‹**ï¼šå…ˆç”¨get_news_flashè·å–å¿«è®¯åˆ—è¡¨ï¼Œå†é’ˆå¯¹é‡è¦äº‹ä»¶ç”¨search_newså’Œfetch_webpageè·å–è¯¦æƒ…

**ç ”ç©¶æµç¨‹å»ºè®®**ï¼š
1. é¦–å…ˆè°ƒç”¨ get_current_time ç¡®è®¤å½“å‰æ—¶é—´
2. æ”¶é›†åŸºç¡€æ•°æ®ï¼ˆæŒ‡æ•°ã€æ¿å—ã€åŸºé‡‘ã€é»„é‡‘ç­‰ï¼‰
3. è°ƒç”¨ get_news_flash è·å–7Ã—24å¿«è®¯åˆ—è¡¨
4. **ã€å¯é€‰æ­¥éª¤ - æ·±åº¦è§£è¯»ã€‘** é’ˆå¯¹å¿«è®¯ä¸­çš„é‡è¦äº‹ä»¶ï¼š
   - ä½¿ç”¨ search_news æœç´¢ç›¸å…³çš„è¯¦ç»†æŠ¥é“ï¼ˆå¦‚ï¼š"æ”¿ç­–åç§° è¯¦æƒ…"ã€"äº‹ä»¶å å¸‚åœºå½±å“"ï¼‰
   - ä½¿ç”¨ fetch_webpage è·å–å®Œæ•´æ–‡ç« å†…å®¹ï¼Œæ·±å…¥äº†è§£äº‹ä»¶èƒŒæ™¯
5. ç»¼åˆæ‰€æœ‰æ•°æ®ï¼Œç”Ÿæˆ**æ•°æ®è¯¦å®ã€åˆ†ææ·±å…¥ã€é£é™©å……åˆ†æç¤º**çš„æŠ¥å‘Š

**å¯ç”¨å·¥å…·**ï¼š
{tools}

å·¥å…·åç§°: {tool_names}

**æŠ¥å‘Šç”Ÿæˆè¦æ±‚**ï¼š

ä½ æ˜¯ä¸€ä½èµ„æ·±é‡‘èåˆ†æå¸ˆï¼Œéœ€è¦ç”Ÿæˆä¸€ä»½**è¯¦å°½çš„ä¸“ä¸šè¡Œä¸šç ”ç©¶æŠ¥å‘Š**ã€‚

**æ ¸å¿ƒè¦æ±‚**ï¼š
1. â­ **æŠ¥å‘Šæ€»å­—æ•°å¿…é¡»è¾¾åˆ°10000å­—ä»¥ä¸Š** - è¿™æ˜¯æœ€é‡è¦çš„è¦æ±‚
2. ğŸ“Š **å†…å®¹å¿…é¡»è¯¦å®æ·±å…¥** - æ¯ä¸ªåˆ†æç‚¹éƒ½è¦å±•å¼€è¯¦ç»†è®ºè¿°ï¼Œä¸èƒ½æµ…å°è¾„æ­¢
3. ğŸ” **æ•°æ®æ”¯æ’‘å……åˆ†** - æ‰€æœ‰åˆ¤æ–­å¿…é¡»æœ‰å…·ä½“æ•°æ®å’Œæ¡ˆä¾‹æ”¯æŒ
4. ğŸ“ˆ **æ ¼å¼ä¸°å¯Œæ¸…æ™°** - ä½¿ç”¨è¡¨æ ¼ã€åˆ—è¡¨ã€åŠ ç²—ç­‰Markdownæ ¼å¼å¢å¼ºå¯è¯»æ€§

**æ·±åº¦è§£è¯»å»ºè®®**ï¼š
- é’ˆå¯¹7Ã—24å¿«è®¯ä¸­çš„é‡è¦äº‹ä»¶ï¼Œå¯ä½¿ç”¨ search_news æœç´¢ç›¸å…³è¯¦ç»†æŠ¥é“
- å¯¹æœç´¢åˆ°çš„é‡è¦æ–‡ç« ï¼Œå¯ä½¿ç”¨ fetch_webpage è·å–åŸæ–‡å®Œæ•´å†…å®¹
- ç»“åˆå¿«è®¯ä¿¡æ¯å’Œè¯¦ç»†æŠ¥é“ï¼Œæä¾›æ›´æ·±å…¥çš„åˆ†æå’Œè§£è¯»

**æŠ¥å‘Šå†…å®¹å»ºè®®**ï¼ˆä½ å¯ä»¥è‡ªç”±å‘æŒ¥ï¼Œä¸å¿…ä¸¥æ ¼éµå¾ªï¼‰ï¼š
- å®è§‚å¸‚åœºç¯å¢ƒï¼ˆå…¨çƒå¸‚åœºè”åŠ¨ã€Aè‚¡æŠ€æœ¯é¢ã€æˆäº¤é‡åˆ†æã€å¸‚åœºæƒ…ç»ªï¼‰
- é‡å¤§äº‹ä»¶æ·±åº¦è§£è¯»ï¼ˆæ¯ä¸ªé‡è¦å¿«è®¯éƒ½è¦è¯¦ç»†åˆ†æ500-1000å­—ï¼šäº‹ä»¶èƒŒæ™¯+å¸‚åœºå½±å“+æŠ•èµ„å¯ç¤ºï¼‰
- è¡Œä¸šæ¿å—æœºä¼šæŒ–æ˜ï¼ˆå¼ºåŠ¿æ¿å—çš„é©±åŠ¨å› ç´ ã€æŒç»­æ€§åˆ¤æ–­ã€é¾™å¤´æ ‡çš„åˆ†æï¼Œæ¯ä¸ªæ¿å—300-500å­—ï¼‰
- å¼±åŠ¿æ¿å—é£é™©æç¤ºï¼ˆä¸‹è·ŒåŸå› ã€åº•éƒ¨åˆ¤æ–­ã€åå¼¹æ—¶æœºï¼‰
- åŸºé‡‘ç»„åˆè¯Šæ–­ï¼ˆæ¯åªæŒä»“åŸºé‡‘çš„è¯¦ç»†åˆ†æï¼šä¸šç»©ã€æŒä»“ã€é£é™©ã€æ“ä½œå»ºè®®ï¼Œæ¯åª500-800å­—ï¼‰
- è°ƒä»“å»ºè®®ï¼ˆæ¨èåŸºé‡‘+ç†ç”±+é£é™©æç¤ºï¼Œæ¯ä¸ªæ¨è300-500å­—ï¼‰
- å¤šç»´åº¦é£é™©åˆ†æï¼ˆç³»ç»Ÿæ€§é£é™©ã€æ”¿ç­–é£é™©ã€å¸‚åœºæƒ…ç»ªã€è¡Œä¸šé£é™©ï¼Œæ¯ç±»é£é™©300-500å­—ï¼‰
- æŠ•èµ„ç­–ç•¥ï¼ˆçŸ­æœŸ/ä¸­æœŸ/é•¿æœŸç­–ç•¥ï¼Œå…·ä½“å¯æ‰§è¡Œçš„æ“ä½œè®¡åˆ’ï¼‰
- ä¿¡æ¯æ¥æºè¯´æ˜ï¼ˆåˆ—å‡ºæ‰€æœ‰ä½¿ç”¨çš„å·¥å…·å’Œæ•°æ®æ¥æºï¼‰

**æ ¼å¼å»ºè®®**ï¼ˆMarkdownï¼‰ï¼š
- ä½¿ç”¨è¡¨æ ¼å±•ç¤ºç»“æ„åŒ–æ•°æ®ï¼ˆæŒ‡æ•°ã€æ¿å—ã€åŸºé‡‘ç­‰ï¼‰
- ä½¿ç”¨åˆ—è¡¨ç»„ç»‡è¦ç‚¹
- ä½¿ç”¨åŠ ç²—çªå‡ºå…³é”®ä¿¡æ¯
- ä½¿ç”¨å¼•ç”¨å—çªå‡ºæ ¸å¿ƒç»“è®º
- ä½¿ç”¨åˆ†éš”çº¿åˆ†éš”ç« èŠ‚
- é€‚å½“ä½¿ç”¨Emojiå¢å¼ºå¯è¯»æ€§

**å†™ä½œé£æ ¼**ï¼š
- ä¸“ä¸šä¸¥è°¨ï¼Œæ•°æ®è¯¦å®
- é€»è¾‘æ¸…æ™°ï¼Œå±‚æ¬¡åˆ†æ˜
- è¯­è¨€æµç•…ï¼Œæ˜“äºç†è§£
- åˆ†ææ·±å…¥ï¼Œè§è§£ç‹¬åˆ°
- **é‡ç‚¹ï¼šå†…å®¹è¦å……å®ï¼Œä¸è¦æƒœå­—å¦‚é‡‘ï¼Œè¦åƒå†™ä¸€æœ¬å°å†Œå­ä¸€æ ·è¯¦ç»†**

**é‡è¦æç¤º**ï¼š
- æ¯æ¬¡åªè°ƒç”¨ä¸€ä¸ªå·¥å…·ï¼Œè§‚å¯Ÿç»“æœåå†å†³å®šä¸‹ä¸€æ­¥
- å¯ä»¥ä½¿ç”¨ search_news å’Œ fetch_webpage è·å–7Ã—24å¿«è®¯çš„è¯¦ç»†å†…å®¹
- ç¡®ä¿æŠ¥å‘Š**å­—æ•°è¾¾åˆ°10000å­—ä»¥ä¸Š**ï¼Œå†…å®¹è¯¦å®ã€æ•°æ®å……åˆ†ã€å»ºè®®å…·ä½“
- å……åˆ†æç¤ºé£é™©ï¼Œé¿å…è¿‡åº¦ä¹è§‚æˆ–æ‚²è§‚

ä½¿ç”¨ä»¥ä¸‹æ ¼å¼ï¼š

Question: è¦è§£å†³çš„é—®é¢˜
Thought: ä½ åº”è¯¥æ€è€ƒè¦åšä»€ä¹ˆ
Action: è¦é‡‡å–çš„è¡ŒåŠ¨ï¼Œå¿…é¡»æ˜¯ [{tool_names}] ä¸­çš„ä¸€ä¸ª
Action Input: è¡ŒåŠ¨çš„è¾“å…¥
Observation: è¡ŒåŠ¨çš„ç»“æœ
... (è¿™ä¸ª Thought/Action/Action Input/Observation å¯ä»¥é‡å¤Næ¬¡)
Thought: æˆ‘ç°åœ¨çŸ¥é“æœ€ç»ˆç­”æ¡ˆäº†
Final Answer: æœ€ç»ˆç­”æ¡ˆï¼ˆå®Œæ•´çš„markdownæ ¼å¼æŠ¥å‘Šï¼Œå†…å®¹è¯¦å®ï¼Œå­—æ•°10000å­—ä»¥ä¸Šï¼‰

å¼€å§‹ï¼

Question: {input}
Thought: {agent_scratchpad}""")

            # åˆ›å»ºagent
            agent = create_react_agent(
                llm=llm,
                tools=tools,
                prompt=react_prompt
            )

            # åˆ›å»ºexecutor
            agent_executor = AgentExecutor(
                agent=agent,
                tools=tools,
                verbose=True,
                handle_parsing_errors=True,
                max_iterations=50,  # å¢åŠ è¿­ä»£æ¬¡æ•°æ”¯æŒæ›´å¤æ‚çš„ç ”ç©¶æµç¨‹
                max_execution_time=900,  # å¢åŠ åˆ°15åˆ†é’Ÿï¼Œæ”¯æŒæ·±åº¦ç½‘é¡µæŠ“å–
                return_intermediate_steps=True  # è¿”å›ä¸­é—´æ­¥éª¤ï¼Œé˜²æ­¢è¶…æ—¶æ—¶ä¸¢å¤±å†…å®¹
            )

            # æ‰§è¡Œæ·±åº¦ç ”ç©¶
            logger.info("ğŸ” Agentå¼€å§‹è‡ªä¸»ç ”ç©¶å’Œæ•°æ®æ”¶é›†...")

            result = agent_executor.invoke({
                "input": f"""è¯·ç”Ÿæˆä¸€ä»½å…³äºå½“å‰å¸‚åœºï¼ˆ{current_date}ï¼‰çš„æ·±åº¦åˆ†ææŠ¥å‘Šã€‚

ã€æŠ¥å‘Šè¦æ±‚ã€‘å¿…é¡»åŒ…å«ä»¥ä¸‹ç« èŠ‚ï¼Œå¹¶ä½¿ç”¨ä¸°å¯Œçš„Markdownæ ¼å¼ï¼ˆè¡¨æ ¼ã€åˆ—è¡¨ã€åŠ ç²—ã€å¼•ç”¨å—ã€Emojiç­‰ï¼‰ï¼š
1. å¸‚åœºæ•´ä½“è¶‹åŠ¿åˆ†æï¼ˆåŒ…å«æŒ‡æ•°è¡¨æ ¼ã€çƒ­ç‚¹åˆ—è¡¨ï¼‰
2. è¡Œä¸šæ¿å—æœºä¼šåˆ†æï¼ˆåŒ…å«é¢†æ¶¨/è·Œæ¿å—è¡¨æ ¼ï¼‰
3. åŸºé‡‘ç»„åˆæŠ•èµ„å»ºè®®ï¼ˆåŒ…å«æŒä»“è¡¨æ ¼ã€è°ƒä»“å»ºè®®åˆ—è¡¨ï¼‰
4. é£é™©æç¤ºä¸åº”å¯¹ï¼ˆåŒ…å«é£é™©è¡¨æ ¼ï¼Œå«ä¿¡æ¯æ¥æºè¯´æ˜ï¼‰

ã€æ·±åº¦è§£è¯»å»ºè®®ã€‘é’ˆå¯¹7Ã—24å¿«è®¯ä¸­çš„é‡è¦äº‹ä»¶ï¼š
- å¯ä½¿ç”¨ search_news æœç´¢ç›¸å…³è¯¦ç»†æŠ¥é“ï¼ˆå¦‚ï¼š"æ”¿ç­–åç§° è¯¦æƒ…"ã€"äº‹ä»¶å åˆ†æ"ï¼‰
- å¯ä½¿ç”¨ fetch_webpage è·å–å®Œæ•´æ–°é—»æ–‡ç« å†…å®¹ï¼Œæ·±å…¥äº†è§£äº‹ä»¶èƒŒæ™¯

åŠ¡å¿…ä½¿ç”¨Markdownè¡¨æ ¼å±•ç¤ºæ‰€æœ‰æ•°æ®ï¼Œç¡®ä¿æŠ¥å‘Šå­—æ•°è¾¾åˆ°10000å­—ä»¥ä¸Šï¼""",
                "current_date": current_date
            })

            # æå–æœ€ç»ˆæŠ¥å‘Šï¼ˆå¢å¼ºå®¹é”™å¤„ç†ï¼‰
            final_report = result.get("output", "")

            # å¦‚æœoutputä¸ºç©ºï¼ˆå¯èƒ½ç”±äºè¶…æ—¶ï¼‰ï¼Œå°è¯•ä»intermediate_stepsæå–å†…å®¹
            if not final_report or final_report == "Agent stopped due to iteration limit or time limit.":
                logger.warning("âš ï¸ Agentæœªè¿”å›å®Œæ•´è¾“å‡ºï¼Œå°è¯•ä»ä¸­é—´æ­¥éª¤æå–å†…å®¹...")
                intermediate_steps = result.get("intermediate_steps", [])

                # æå–æ‰€æœ‰Observationä¸­çš„å†…å®¹
                collected_info = []
                for step in intermediate_steps:
                    if len(step) >= 2:
                        action, observation = step[0], step[1]
                        if observation and isinstance(observation, str) and len(observation) > 50:
                            collected_info.append(f"### {action.tool if hasattr(action, 'tool') else 'æ•°æ®æ”¶é›†'}\n\n{observation}\n")

                if collected_info:
                    final_report = "\n\n".join(collected_info)
                    final_report += "\n\n---\n\nâš ï¸ **æ³¨æ„**ï¼šç”±äºAgentæ‰§è¡Œæ—¶é—´é™åˆ¶ï¼Œæœ¬æŠ¥å‘Šç”±ä¸­é—´æ•°æ®è‡ªåŠ¨ç»„åˆç”Ÿæˆã€‚å»ºè®®å¢åŠ è¿­ä»£æ¬¡æ•°æˆ–æ‰§è¡Œæ—¶é—´é™åˆ¶ã€‚"
                else:
                    final_report = "Agent stopped due to iteration limit or time limit."

            # ç”Ÿæˆå®Œæ•´çš„markdownæ–‡ä»¶
            markdown_content = f"""# ğŸ”¬ AIæ·±åº¦ç ”ç©¶æŠ¥å‘Š

**ç”Ÿæˆæ—¶é—´**ï¼š{time.strftime('%Y-%m-%d %H:%M:%S')}
**ç ”ç©¶æ¨¡å¼**ï¼šReAct Agent è‡ªä¸»ç ”ç©¶

---

{final_report}

---

ğŸ’¡ **æç¤º**ï¼šæœ¬æŠ¥å‘Šç”±AIæ·±åº¦ç ”ç©¶ç”Ÿæˆï¼ŒAgentè‡ªä¸»å†³å®šæ•°æ®æ”¶é›†ç­–ç•¥ã€‚ä»…ä¾›å‚è€ƒï¼Œä¸æ„æˆæŠ•èµ„å»ºè®®ã€‚æŠ•èµ„æœ‰é£é™©ï¼Œå…¥å¸‚éœ€è°¨æ…ã€‚
"""

            # ä¿å­˜markdownæ–‡ä»¶
            if not os.path.exists(report_dir):
                os.makedirs(report_dir, exist_ok=True)

            report_filename = f"{report_dir}/AIå¸‚åœºæ·±åº¦ç ”ç©¶æŠ¥å‘Š{time.strftime('%Y%m%d_%H%M%S')}.md"
            with open(report_filename, "w", encoding="utf-8") as f:
                f.write(markdown_content)

            logger.info(f"âœ… æ·±åº¦ç ”ç©¶æŠ¥å‘Šå·²ä¿å­˜è‡³ï¼š{report_filename}")

            # è¾“å‡ºæŠ¥å‘Šåˆ°æ§åˆ¶å°
            logger.critical(f"{time.strftime('%Y-%m-%d %H:%M')} ğŸ”¬ AIæ·±åº¦ç ”ç©¶æŠ¥å‘Š")
            logger.info("=" * 80)
            for line in self.format_text(final_report, max_width=70):
                logger.info(line)
            logger.info("=" * 80)
            logger.info("ğŸ’¡ æç¤ºï¼šæœ¬æŠ¥å‘Šç”±AIæ·±åº¦ç ”ç©¶ç”Ÿæˆï¼Œä»…ä¾›å‚è€ƒï¼Œä¸æ„æˆæŠ•èµ„å»ºè®®ã€‚")
            logger.info("=" * 80)

        except Exception as e:
            logger.error(f"æ·±åº¦ç ”ç©¶æ¨¡å¼å‡ºé”™: {e}")
            import traceback
            logger.error(traceback.format_exc())

